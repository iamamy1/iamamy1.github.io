(window.webpackJsonp=window.webpackJsonp||[]).push([[19],{391:function(e,a,t){"use strict";t.r(a);var r=t(46),i=Object(r.a)({},(function(){var e=this,a=e.$createElement,t=e._self._c||a;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"graph-neural-networks"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#graph-neural-networks"}},[e._v("#")]),e._v(" Graph Neural Networks")]),e._v(" "),t("h2",{attrs:{id:"_6-1-deep-graph-encoders"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_6-1-deep-graph-encoders"}},[e._v("#")]),e._v(" 6.1 Deep Graph Encoders")]),e._v(" "),t("p",[t("strong",[e._v("deep methods based on graph neural networks（GNNs）")]),e._v("：multiple layers of non-linear transformations based on graph structure")]),e._v(" "),t("ol",[t("li",[e._v("Graph convolutions")]),e._v(" "),t("li",[e._v("Activation function")]),e._v(" "),t("li",[e._v("Reguization")])]),e._v(" "),t("p",[t("strong",[e._v("today‘s outline")])]),e._v(" "),t("ul",[t("li",[e._v("Basics of deep learning")]),e._v(" "),t("li",[e._v("Deep learning for graphs")]),e._v(" "),t("li",[e._v("Graph Convolutional Networks and GraphSAGE")])]),e._v(" "),t("h2",{attrs:{id:"_6-2-basics-of-deep-learning"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_6-2-basics-of-deep-learning"}},[e._v("#")]),e._v(" 6.2 Basics of deep learning")]),e._v(" "),t("p",[t("strong",[e._v("Supervised learning:")]),e._v(" we are given input "),t("em",[e._v("x")]),e._v(", and the goal is to predict label "),t("em",[e._v("y")])]),e._v(" "),t("p",[t("strong",[e._v("Formulate the task as an optimization problem")]),e._v("：")]),e._v(" "),t("ul",[t("li",[e._v("我们定义一个拟合函数(objective function)")]),e._v(" "),t("li",[e._v("然后根据损失函数求出损失，以此来衡量拟合函数的好坏。")])]),e._v(" "),t("h3",{attrs:{id:"loss-function"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#loss-function"}},[e._v("#")]),e._v(" Loss function")]),e._v(" "),t("p",[e._v("损失函数有很多种，比如L2 loss，交叉熵等等")]),e._v(" "),t("p",[e._v("One common loss for classification: "),t("strong",[e._v("cross entropy (CE)")]),e._v("\n$$\nCE(y,f(x))=-∑_{i=1}^C(y_ilogf(x)_i)\n$$\noptimize the objective function : gradient vector(梯度是沿最大增长方向的方向导数)\n$$\n∇_Θℒ=(\\frac{∂ℒ}{∂Θ_1},\\frac{∂ℒ}{∂Θ_2},...)\n$$")]),e._v(" "),t("h3",{attrs:{id:"gradient-descent"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#gradient-descent"}},[e._v("#")]),e._v(" Gradient Descent")]),e._v(" "),t("p",[e._v("可以使用梯度下降对拟合函数进行多次迭代优化")]),e._v(" "),t("ul",[t("li",[e._v("Iterative algorithm: repeatedly update weights in the (opposite) direction of gradients until convergence（梯度一点一点缓慢下降，直到收敛，梯度为0时取得极值）")]),e._v(" "),t("li",[e._v("Ideal termination condition: 0 gradient.  In practice, we stop training if it no longer improves performance on validation set")])]),e._v(" "),t("p",[t("strong",[e._v("Stochastic gradient descent (SGD)")])]),e._v(" "),t("p",[e._v("At every step, pick a different minibatch Β containing a subset of the dataset, use it as input x")]),e._v(" "),t("p",[e._v("minibatch SGD：")]),e._v(" "),t("ul",[t("li",[e._v("Batch size: the number of data points in a minibatch")]),e._v(" "),t("li",[e._v("Iteration: 1 step of SGD on a minibatch")]),e._v(" "),t("li",[e._v("Epoch: one full pass over the dataset(iterations is equal to ratio of dataset size and batch size)")])]),e._v(" "),t("p",[t("strong",[e._v("两种不同的梯度下降方法：")])]),e._v(" "),t("p",[e._v("SGD：匀速，1阶导，表示速度，得到局部最优结果")]),e._v(" "),t("p",[e._v("Adam：2阶导，表示加速度，期望得到全局最优，只是有可能得到全局最优结，可能会导致不收敛，而SGD一定收敛，可以先用Adam，将要收敛时再用SGD")]),e._v(" "),t("h3",{attrs:{id:"back-propagation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#back-propagation"}},[e._v("#")]),e._v(" Back-propagation")]),e._v(" "),t("p",[t("strong",[e._v("Forward propagation")]),e._v(":compute ℒ given x")]),e._v(" "),t("p",[t("strong",[e._v("Back-propagation:")]),e._v(" obtain gradient ∇Θℒ using a chain rule")]),e._v(" "),t("p",[e._v("Use of chain rule to propagate gradients of intermediate steps, and finally obtain gradient")]),e._v(" "),t("p",[t("strong",[e._v("chain rule:")]),e._v("(链式求导法对于矩阵求导仍旧适用)\n$$\n\\frac{dz}{dx}=\\frac{dz}{dx}·\\frac{dy}{dx}\n$$")]),e._v(" "),t("h3",{attrs:{id:"non-linearity"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#non-linearity"}},[e._v("#")]),e._v(" non-linearity")]),e._v(" "),t("ul",[t("li",[e._v("Rectified linear unit (ReLU)\t  ReLU(x) = max(x, 0)")]),e._v(" "),t("li",[e._v("Sigmoid")])]),e._v(" "),t("p",[e._v("$$\nσ(x)=\\frac{1}{1+e^{-x}}\n$$")]),e._v(" "),t("p",[t("strong",[e._v("Multi-layer Perceptron(MLP)")])]),e._v(" "),t("p",[e._v("Each layer of MLP combines linear transformation and non-linearity:\n$$\nx^{l+1}=σ(W_lW_lx^l+b^l)\n$$")]),e._v(" "),t("ul",[t("li",[e._v("where "),t("em",[e._v("Wl")]),e._v(" is weight matrix that transforms hidden representation at layer "),t("em",[e._v("l")]),e._v(" to layer "),t("em",[e._v("l + 1")])]),e._v(" "),t("li",[t("em",[e._v("bl")]),e._v(" is bias at layer "),t("em",[e._v("l")]),e._v(", and is added to the linear transformation of "),t("em",[e._v("x")])]),e._v(" "),t("li",[t("em",[e._v("σ")]),e._v(" is non-linearity function")])]),e._v(" "),t("h3",{attrs:{id:"mlp-多层感知机"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#mlp-多层感知机"}},[e._v("#")]),e._v(" MLP(多层感知机)")]),e._v(" "),t("h2",{attrs:{id:"_6-3-deep-learning-for-graphs"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_6-3-deep-learning-for-graphs"}},[e._v("#")]),e._v(" 6.3 Deep learning for graphs")]),e._v(" "),t("p",[t("strong",[e._v("Local network neighborhoods:")])]),e._v(" "),t("ul",[t("li",[e._v("Describe aggregation strategies")]),e._v(" "),t("li",[e._v("Define computation graphs")])]),e._v(" "),t("p",[t("strong",[e._v("Stacking multiple layers:")])]),e._v(" "),t("ul",[t("li",[e._v("Describe the model, parameters, training")]),e._v(" "),t("li",[e._v("How to fit the model?")]),e._v(" "),t("li",[e._v("Simple example for unsupervised and supervised training")])]),e._v(" "),t("h3",{attrs:{id:"a-naive-approach"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#a-naive-approach"}},[e._v("#")]),e._v(" A Naive Approach")]),e._v(" "),t("ul",[t("li",[e._v("Join adjacency matrix and features")]),e._v(" "),t("li",[e._v("Feed them into a deep neural net")])]),e._v(" "),t("img",{attrs:{src:e.$withBase("/img/GraphsML_img/image-20211009082856331.png"),alt:"mixureSecure"}}),e._v(" "),t("p",[e._v("issue：")]),e._v(" "),t("ol",[t("li",[t("p",[e._v("O(|V|) parameters(参数量大)")])]),e._v(" "),t("li",[t("p",[e._v("Not applicable to graphs of different sizes")])]),e._v(" "),t("li",[t("p",[e._v("Sensitive to node ordering")])])]),e._v(" "),t("h3",{attrs:{id:"convolutional-networks"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#convolutional-networks"}},[e._v("#")]),e._v(" Convolutional Networks")]),e._v(" "),t("p",[e._v("Node’s neighborhood defines a computation graph")]),e._v(" "),t("img",{attrs:{src:e.$withBase("/img/GraphsML_img/image-20211009084301858.png"),alt:"mixureSecure"}}),e._v(" "),t("h3",{attrs:{id:"aggregate-neighborhood"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#aggregate-neighborhood"}},[e._v("#")]),e._v(" Aggregate Neighborhood")]),e._v(" "),t("p",[e._v("Generate node embeddings based on local network neighborhoods")]),e._v(" "),t("ul",[t("li",[e._v("Network neighborhood defines a computation graph")]),e._v(" "),t("li",[e._v("Every node defines a computation graph based on its neighborhood")])]),e._v(" "),t("img",{attrs:{src:e.$withBase("/img/GraphsML_img/image-20211009084514504.png"),alt:"mixureSecure"}}),e._v(" "),t("p",[e._v("Basic approach: Average neighbor messages and apply a neural network")]),e._v(" "),t("img",{attrs:{src:e.$withBase("/img/GraphsML_img/image-20211009085726595.png"),alt:"mixureSecure"}}),e._v(" "),t("ul",[t("li",[e._v("ℎv(l)：the hidden representation of node v at layer l")]),e._v(" "),t("li",[e._v("Wk: weight matrix for neighborhood aggregation")]),e._v(" "),t("li",[e._v("Bk: weight matrix for transforming hidden vector of self（一个偏置项）")])]),e._v(" "),t("h3",{attrs:{id:"model-design"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#model-design"}},[e._v("#")]),e._v(" Model Design")]),e._v(" "),t("ul",[t("li",[e._v("Define a neighborhood aggregation function（聚合函数）")]),e._v(" "),t("li",[e._v("Define a loss function on the embeddings（损失函数）")]),e._v(" "),t("li",[e._v("Train on a set of nodes, i.e., a batch of compute graphs（训练）")]),e._v(" "),t("li",[e._v("Generate embeddings for nodes as needed（生成嵌入）")])]),e._v(" "),t("h2",{attrs:{id:"_6-4-graph-convolutional-networks-and-graphsage"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_6-4-graph-convolutional-networks-and-graphsage"}},[e._v("#")]),e._v(" 6.4 Graph Convolutional Networks and GraphSAGE")]),e._v(" "),t("p",[t("strong",[e._v("GraphSAGE Idea")])]),e._v(" "),t("p",[e._v("GraphSAGE Idea 1 : Any differentiable function that maps set of vectors in N(u) to a single vector")]),e._v(" "),t("p",[e._v("GraphSAGE Idea 2 : Apply L2 normalization to hv(l+1) embedding at every layer")])])}),[],!1,null,null,null);a.default=i.exports}}]);
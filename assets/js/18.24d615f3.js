(window.webpackJsonp=window.webpackJsonp||[]).push([[18],{390:function(e,i,n){"use strict";n.r(i);var a=n(46),t=Object(a.a)({},(function(){var e=this,i=e.$createElement,n=e._self._c||i;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"node-embeddings"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#node-embeddings"}},[e._v("#")]),e._v(" Node Embeddings")]),e._v(" "),n("p",[n("em",[n("strong",[e._v("Core idea:")]),e._v(" Embed nodes so that distances in embedding space reflect node similarities in the original network.")])]),e._v(" "),n("h2",{attrs:{id:"_3-1-graph-representation-learning"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-graph-representation-learning"}},[e._v("#")]),e._v(" 3.1 Graph Representation Learning")]),e._v(" "),n("p",[e._v("alleviates the need to do feature engineering every single time")]),e._v(" "),n("p",[n("strong",[e._v("Goal:")]),e._v(" Efficient task-independent feature learning for machine learning with graphs")]),e._v(" "),n("p",[n("strong",[e._v("Embedding Task:")]),e._v(" map nodes into an embedding space")]),e._v(" "),n("h2",{attrs:{id:"_3-2-node-embeddings-encoder-and-decoder"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-node-embeddings-encoder-and-decoder"}},[e._v("#")]),e._v(" 3.2 Node Embeddings: Encoder and Decoder")]),e._v(" "),n("p",[n("strong",[e._v("Goal")]),e._v(" is to encode nodes so that similarity in the embedding space approximates similarity in the graph")]),e._v(" "),n("ol",[n("li",[n("u",[e._v("Encoder")]),e._v(" maps from nodes to embeddings")]),e._v(" "),n("li",[e._v("Define a node "),n("u",[e._v("similarity function")])]),e._v(" "),n("li",[n("u",[e._v("Decoder")]),e._v(" maps from embeddings to the similarity score")]),e._v(" "),n("li",[n("u",[e._v("Optimize the parameters of the encoder")]),e._v(" so that  similarity in the embedding space approximates similarity in the original network")])]),e._v(" "),n("p",[n("strong",[e._v("Two Key Components")])]),e._v(" "),n("ul",[n("li",[n("strong",[e._v("Encoder:")]),e._v(" maps each node to a low-dimensional vector")]),e._v(" "),n("li",[n("strong",[e._v("Similarity function:")]),e._v(" specifies how the relationships in vector space map to the relationships in the original network")])]),e._v(" "),n("p",[n("strong",[e._v('"Shallow" Encodeing')])]),e._v(" "),n("ul",[n("li",[e._v("Simplest encoding approach: Encoder is just an embedding-lookup")]),e._v(" "),n("li",[e._v("Each node is assigned a unique embedding vector")])]),e._v(" "),n("p",[n("strong",[e._v("Encoder + Decoder Framework")])]),e._v(" "),n("ul",[n("li",[e._v("Shallow encoder: embedding lookup")]),e._v(" "),n("li",[e._v("Optimize parameters")]),e._v(" "),n("li",[e._v("Decoder: based on node similarity")])]),e._v(" "),n("h2",{attrs:{id:"_3-3-random-walk-approaches-for-node-embeddings"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-random-walk-approaches-for-node-embeddings"}},[e._v("#")]),e._v(" 3.3 Random Walk Approaches for Node Embeddings")]),e._v(" "),n("p",[n("strong",[e._v("Non-linear functions used to produce predicted probabilities")])]),e._v(" "),n("ul",[n("li",[e._v("Softmax function: Turns vector of K real values (model predictions) into K probabilities that sum to 1")]),e._v(" "),n("li",[e._v("Sigmoid function: S-shaped function that turns real values into the range of (0, 1)")])]),e._v(" "),n("p",[n("strong",[e._v("Random Walk")])]),e._v(" "),n("ul",[n("li",[e._v("Given a graph and a starting point, we select a neighbor of it at random, and move to this neighbor; then we select a neighbor of this point at random, and move to it, etc. The (random) sequence of points visited this way is a random walk on the graph")])]),e._v(" "),n("p",[n("strong",[e._v("Unsupervised Feature Learning")])]),e._v(" "),n("p",[n("strong",[e._v("Random Walk Optimization")])]),e._v(" "),n("ol",[n("li",[e._v("Run short fixed-length random walks: starting from each node u in the graph using some random walk strategy "),n("em",[e._v("R")])]),e._v(" "),n("li",[e._v("For each node u collect N_R(u) , the multiset* of nodes visited on random walks starting from u")]),e._v(" "),n("li",[e._v("Optimize embeddings according to: Given node u, predict its neighbors N_R(u)")])]),e._v(" "),n("p",[e._v("But doing this naively is too expensive, especially the normalization term from the softmax,  so we have to approximate it, the solution is negative sampling")]),e._v(" "),n("p",[n("strong",[e._v("Negative Sampling")])]),e._v(" "),n("ul",[n("li",[e._v("Sample K negative nodes each with prob. proportional to its degree")]),e._v(" "),n("li",[e._v("Two considerations for K\n"),n("ol",[n("li",[e._v("Higher K gives more robust estimates")]),e._v(" "),n("li",[e._v("Higher K corresponds to higher bias on negative events in practice")])])])]),e._v(" "),n("p",[n("strong",[e._v("Stochastic Gradient Descent(Optimize embeddings)")])]),e._v(" "),n("ul",[n("li",[n("strong",[e._v("Gradient Descent")]),e._v(": a simple way to minimize objective function")])]),e._v(" "),n("ol",[n("li",[e._v("Initialize Z_i at some randomized value for all i")]),e._v(" "),n("li",[e._v("Iterate until convergence")])]),e._v(" "),n("ul",[n("li",[n("strong",[e._v("Stochastic Gradient Descent")]),e._v(": Instead of evaluating gradients over all examples, evaluate it for each individual training example")])]),e._v(" "),n("ol",[n("li",[e._v("Initialize Z_i at some randomized value for all i")]),e._v(" "),n("li",[e._v("Iterate until convergence")])]),e._v(" "),n("h3",{attrs:{id:"random-walk-strategies"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#random-walk-strategies"}},[e._v("#")]),e._v(" Random walk strategies")]),e._v(" "),n("p",[n("strong",[e._v("node2vec: Biased Walks")])]),e._v(" "),n("ul",[n("li",[e._v("use flexible, biased random walks that can trade off between local and global views of the network")]),e._v(" "),n("li",[e._v("Two classic strategie:\n"),n("ol",[n("li",[e._v("BFS : Micro-view of neighbourhood")]),e._v(" "),n("li",[e._v("DFS : Macro-view of neighbourhood")])])])]),e._v(" "),n("p",[n("strong",[e._v("node2vec algorithm")])]),e._v(" "),n("ol",[n("li",[e._v("Compute random walk probabilities")]),e._v(" "),n("li",[e._v("Simulate R random walks of length L starting from each node u")]),e._v(" "),n("li",[e._v("Optimize the node2vec objective using Stochastic Gradient Descent")])]),e._v(" "),n("ul",[n("li",[e._v("Linear-time complexity")]),e._v(" "),n("li",[e._v("All 3 steps are individually parallelizable")])]),e._v(" "),n("p",[n("strong",[e._v("Other Random Walk Ideas")])]),e._v(" "),n("ul",[n("li",[n("p",[e._v("Different kinds of biased random walks:")]),e._v(" "),n("ol",[n("li",[n("p",[e._v("Based on node attributes")])]),e._v(" "),n("li",[n("p",[e._v("Based on learned weights")])])])]),e._v(" "),n("li",[n("p",[e._v("Alternative optimization schemes:")]),e._v(" "),n("ol",[n("li",[e._v("Directly optimize based on 1-hop and 2-hop random walk probabilities")])])]),e._v(" "),n("li",[n("p",[e._v("Network preprocessing techniques:")]),e._v(" "),n("ol",[n("li",[e._v("Run random walks on modified versions of the original network")])])])]),e._v(" "),n("h2",{attrs:{id:"_3-4-embedding-entire-graphs"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-embedding-entire-graphs"}},[e._v("#")]),e._v(" 3.4 Embedding Entire Graphs")]),e._v(" "),n("p",[n("strong",[e._v("Goal:")]),e._v(" Want to embed a subgraph or an entire graph")]),e._v(" "),n("p",[n("strong",[e._v("Tasks:")])]),e._v(" "),n("ul",[n("li",[e._v("Classifying toxic vs. non-toxic molecules")]),e._v(" "),n("li",[e._v("Identifying anomalous graphs")])]),e._v(" "),n("p",[n("strong",[e._v("Approach 1:Embed nodes and sum/avg them")])]),e._v(" "),n("ul",[n("li",[e._v("Run a standard graph embedding technique "),n("em",[e._v("on")]),e._v(" the (sub)graph G")]),e._v(" "),n("li",[e._v("Then just sum (or average) the node embeddings in the (sub)graph G")])]),e._v(" "),n("p",[n("strong",[e._v("Approach 2:Create super-node that spans the (sub) graph and then embed that node")])]),e._v(" "),n("ul",[n("li",[e._v("Introduce a “virtual node” to represent the (sub)graph and run a standard graph embedding technique")])]),e._v(" "),n("p",[n("strong",[e._v("Approach 3: Anonymous Walk Embeddings")])]),e._v(" "),n("ul",[n("li",[n("p",[e._v("States in anonymous walks correspond to the index of the first time we visited the node in a random walk")])]),e._v(" "),n("li",[n("p",[e._v("Agnostic to the identity of the nodes visited (hence anonymous)")])]),e._v(" "),n("li",[n("p",[e._v("Number of anonymous walks grows exponentially(There are 5 anonymous walks of length 3: 111, 112, 121, 122, 123)")])]),e._v(" "),n("li",[n("p",[e._v("New idea: Learn Walk Embedding: Rather than simply represent each walk by the fraction of times it occurs, we learn embedding Z_i of anonymous walk W_i")])])]),e._v(" "),n("ol",[n("li",[e._v("Idea 1: Sample the anon. walks and represent the graph as fraction of times each anon walk occurs")]),e._v(" "),n("li",[e._v("Idea 2: Embed anonymous walks, concatenate their embeddings to get a graph embedding")])]),e._v(" "),n("h2",{attrs:{id:"_3-5-summary"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-summary"}},[e._v("#")]),e._v(" 3.5 Summary")]),e._v(" "),n("p",[e._v("We discussed graph representation learning, a way to learn node and graph embeddings for downstream tasks, without feature engineering.")]),e._v(" "),n("p",[n("strong",[e._v("Encoder-decoder framework:")])]),e._v(" "),n("ul",[n("li",[e._v("Encoder: embedding lookup")]),e._v(" "),n("li",[e._v("Decoder: predict score based on embedding to match node similarity")])]),e._v(" "),n("p",[n("strong",[e._v("Node similarity measure: (biased) random walk")])]),e._v(" "),n("ul",[n("li",[e._v("Examples: DeepWalk, Node2Vec")])]),e._v(" "),n("p",[n("strong",[e._v("Extension to Graph embedding")])]),e._v(" "),n("ul",[n("li",[e._v("Node embedding aggregation")]),e._v(" "),n("li",[e._v("Anonymous Walk Embeddings")])])])}),[],!1,null,null,null);i.default=t.exports}}]);